# pip install beautifulsoup4 loguru duckdb
# writemdict -> https://github.com/skywind3000/writemdict
# ======================================================================
#
# eccedict.py -
#
# Created by H1DDENADM1N on 2025/01/09
# Last Modified: 2025/01/12 19:52
#
# ======================================================================
import re
import sys
from datetime import datetime
from pathlib import Path

import duckdb
from bs4 import BeautifulSoup
from loguru import logger

from writemdict.writemdict import MDictWriter

# æ”¯æŒå¸¸è§çš„è¯æ€§ç¼©å†™
POS_PATTERN = re.compile(
    r"^(a|na|n|un|v|vt|vi|adj|adv|pron|prep|conj|interj|art|num|aux|pl|sing|past|pp|pres|ger|det|modal|part|suf|pref|abbr|coll|phr)\.\s*(.*)"
)
# å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä¸­æ–‡å­—ç¬¦
CHINESE_PATTERN = re.compile(r"[\u4e00-\u9fff]")


def configure_logging(log_dir, rotation="1 week", retention="1 month", level="DEBUG"):
    """
    é…ç½®æ—¥å¿—è®°å½•å™¨ä»¥è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶ï¼ŒåŒæ—¶ä¿ç•™é¢œè‰²ï¼Œå¹¶æ ¹æ®å½“å‰æ—¥æœŸå’Œæ—¥å¿—çº§åˆ«ç”Ÿæˆæ–‡ä»¶åã€‚

    :param log_dir: æ—¥å¿—æ–‡ä»¶çš„ç›®å½•ã€‚
    :param rotation: æ—¥å¿—æ–‡ä»¶çš„è½®è½¬ç­–ç•¥ã€‚
    :param retention: æ—¥å¿—æ–‡ä»¶çš„ä¿ç•™ç­–ç•¥ã€‚
    :param level: æ—¥å¿—çº§åˆ«ã€‚
    """
    # ä½¿ç”¨ pathlib åˆ›å»ºæ—¥å¿—ç›®å½•è·¯å¾„
    log_path = Path(log_dir)
    log_path.mkdir(parents=True, exist_ok=True)

    # è·å–å½“å‰æ—¥æœŸ
    current_date = datetime.now().strftime("%Y-%m-%d")

    # é…ç½®æ§åˆ¶å°æ—¥å¿—è®°å½•å™¨ï¼Œæ˜¾ç¤ºå¸¦é¢œè‰²çš„æ—¥å¿—
    logger.remove()  # ç§»é™¤é»˜è®¤çš„æ—¥å¿—è®°å½•å™¨
    logger.add(
        sys.stdout,
        colorize=True,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{message}</cyan>",
        level=level,
    )

    # é…ç½®æ–‡ä»¶æ—¥å¿—è®°å½•å™¨ï¼Œæ ¹æ®æ—¥å¿—çº§åˆ«ç”Ÿæˆä¸åŒçš„æ—¥å¿—æ–‡ä»¶
    log_levels = ["TRACE", "DEBUG", "INFO", "SUCCESS", "WARNING", "ERROR", "CRITICAL"]
    for log_level in log_levels:
        log_file_path = log_path / f"{current_date}_{log_level}.log"
        logger.add(
            str(log_file_path),
            colorize=True,
            format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
            level=log_level,
            rotation=rotation,
            retention=retention,
            filter=lambda record, level=log_level: record["level"].name == level,
        )


def convert_csv_to_stardictdb(csv_file: Path, stardictdb_file: Path):
    """
    å­—å…¸è½¬åŒ– stardict.csv è½¬æ¢åˆ° stardict.ddb
    """
    if not csv_file.exists():
        logger.error(f"{csv_file} æœªæ‰¾åˆ°")
        raise FileNotFoundError(f"{csv_file} æœªæ‰¾åˆ°")
    if stardictdb_file.exists():
        logger.error(f"{stardictdb_file} å·²å­˜åœ¨")
        raise FileExistsError(f"{stardictdb_file} å·²å­˜åœ¨")
    # è¿æ¥åˆ° stardict.ddb æ•°æ®åº“ï¼ˆå¦‚æœæ•°æ®åº“ä¸å­˜åœ¨ï¼Œåˆ™ä¼šè‡ªåŠ¨åˆ›å»ºï¼‰
    conn = duckdb.connect(database=str(stardictdb_file), read_only=False)
    # è¯»å–CSVæ–‡ä»¶å¹¶å¯¼å…¥åˆ° stardict.ddb æ•°æ®åº“çš„ stardict è¡¨ä¸­
    conn.execute(f"CREATE TABLE stardict AS SELECT * FROM read_csv_auto('{csv_file}')")
    conn.close()


def update_phonetics_from_phoneticsdb_to_stardictdb(
    phoneticsdb_file: Path, stardictdb_file: Path
):
    """
    ä» phonetics.ddb æ–‡ä»¶ä¸­ phon_uk å’Œ phon_us åˆ—è¯»å–è‹±ã€ç¾éŸ³æ ‡æ•°æ®ï¼Œå¹¶æ›´æ–°åˆ° stardict.ddb æ–‡ä»¶ä¸­çš„ phonetic åˆ—
    """
    if not stardictdb_file.exists():
        logger.error(f"{stardictdb_file} æœªæ‰¾åˆ°")
        raise FileNotFoundError(f"{stardictdb_file} æœªæ‰¾åˆ°")
    if not phoneticsdb_file.exists():
        logger.error(f"{phoneticsdb_file} æœªæ‰¾åˆ°")
        raise FileNotFoundError(f"{phoneticsdb_file} æœªæ‰¾åˆ°")

    # è¿æ¥åˆ° stardict.ddb æ•°æ®åº“
    conn = duckdb.connect(database=str(stardictdb_file), read_only=False)
    cursor = conn.cursor()
    # è¿æ¥åˆ° phonetics.ddb æ•°æ®åº“
    phonetics_conn = duckdb.connect(database=str(phoneticsdb_file), read_only=True)
    phonetics_cursor = phonetics_conn.cursor()
    # ä» stardict.ddb æ‹¿ word å» phonetics.ddb å–éŸ³æ ‡
    try:
        # ä» stardict.ddb ä¸­è·å–æ‰€æœ‰å•è¯
        cursor.execute("SELECT word, phonetic FROM stardict")
        rows = cursor.fetchall()

        # éå†æ¯ä¸ªå•è¯ï¼Œä» phonetics.ddb ä¸­è·å–éŸ³æ ‡å¹¶æ›´æ–°åˆ° stardict.ddb
        for row in rows:
            (word, phonetic) = row
            phonetics_cursor.execute(
                """
                SELECT phon_uk, phon_us
                FROM words
                WHERE word = ?
            """,
                (word,),
            )
            result = phonetics_cursor.fetchone()

            if result:
                (phon_uk, phon_us) = result
                # æ›´æ–° stardict.ddb ä¸­çš„ phonetic åˆ—
                if phon_uk == phon_us:
                    new_phonetic = phon_uk.strip("/")
                else:
                    new_phonetic = f"è‹± {phon_uk.strip('/')} ç¾ {phon_us.strip('/')}"
                cursor.execute(
                    """
                    UPDATE stardict
                    SET phonetic = ?
                    WHERE word = ?
                """,
                    (new_phonetic, word),
                )
                logger.debug(f"æ›´æ–°å•è¯ {word} çš„éŸ³æ ‡: {new_phonetic}")
            else:
                pass  # è·³è¿‡æ²¡æœ‰æ‰¾åˆ°éŸ³æ ‡çš„å•è¯

        # æäº¤äº‹åŠ¡
        conn.commit()

    except Exception as e:
        logger.error(f"æ›´æ–°éŸ³æ ‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        if conn.in_transaction:  # æ£€æŸ¥æ˜¯å¦æœ‰æ´»åŠ¨çš„äº‹åŠ¡
            conn.rollback()  # å›æ»šäº‹åŠ¡
    finally:
        # å…³é—­è¿æ¥
        cursor.close()
        conn.close()
        phonetics_cursor.close()
        phonetics_conn.close()


def build_phonetics_ddb(oald_txt: Path, phoneticsdb_file: Path):
    """
    ä» oald-fork.txt æ–‡ä»¶ä¸­è¯»å–å•è¯å’Œè‹±ã€ç¾éŸ³æ ‡æ•°æ®ï¼Œå¹¶ç”Ÿæˆ phonetics.ddb æ•°æ®åº“ï¼ˆwords è¡¨åŒ…å« wordã€phon_ukã€phon_us ä¸‰ä¸ªåˆ—ï¼‰
    """
    if not oald_txt.exists():
        logger.error(f"{oald_txt} æœªæ‰¾åˆ°")
        raise FileNotFoundError(f"{oald_txt} æœªæ‰¾åˆ°")

    # è¿æ¥åˆ° phonetics.ddb æ•°æ®åº“
    conn = duckdb.connect(phoneticsdb_file)
    cursor = conn.cursor()

    # åˆ›å»ºè¡¨æ¥å­˜å‚¨å•è¯å’ŒéŸ³æ ‡ä¿¡æ¯
    cursor.execute("""
        CREATE TABLE words (
            word TEXT PRIMARY KEY,
            phon_uk TEXT,
            phon_us TEXT
        )
    """)

    with oald_txt.open("r", encoding="utf-8") as f:
        word = None  # å½“å‰è¯æ¡
        # é€è¡Œè¯»å–æ–‡ä»¶
        for line in f:
            line = line.strip()  # å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦
            if not line:
                continue  # è·³è¿‡ç©ºè¡Œ
            # å¤„ç† "</>" è¡Œ
            if line.startswith("</>"):
                word = None  # é‡ç½®å½“å‰è¯æ¡
                continue
            # å¤„ç† "@@@LINK=" è¡Œ
            elif line.startswith("@@@LINK="):
                word = None  # é‡ç½®å½“å‰è¯æ¡
                continue  # è·³è¿‡é“¾æ¥è¡Œï¼Œå…ˆå»ºç«‹éé“¾æ¥è¯æ¡ï¼Œç¨åå†å¤„ç†é“¾æ¥è¯æ¡
            # å¤„ç†æ™®é€šè¯æ¡è¡Œ
            elif line.startswith("<link href="):
                if word is None:
                    logger.debug(f"æœªæ‰¾åˆ°ä¸è¡Œ '{line}' å¯¹åº”çš„å•è¯ï¼Œè·³è¿‡")
                    continue
                # æå–è‹±å¼éŸ³æ ‡
                soup = BeautifulSoup(line, "html.parser")
                phon_uk = soup.find("div", class_="phons_br")
                phon_uk = (
                    phon_uk.find("span", class_="phon").text.strip()
                    if phon_uk
                    else None
                )
                # æå–ç¾å¼éŸ³æ ‡
                phon_us = soup.find("div", class_="phons_n_am")
                phon_us = (
                    phon_us.find("span", class_="phon").text.strip()
                    if phon_us
                    else None
                )
                if phon_uk is not None and phon_us is not None:
                    # å°†å•è¯å’ŒéŸ³æ ‡ä¿¡æ¯æ’å…¥ phonetics.ddb æ•°æ®åº“
                    try:
                        cursor.execute(
                            """
                            INSERT INTO words (word, phon_uk, phon_us)
                            VALUES (?, ?, ?)
                        """,
                            (word, phon_uk, phon_us),
                        )
                    except duckdb.duckdb.ConstraintException:
                        logger.debug(f"è¯æ¡ {word} å·²å­˜åœ¨ï¼Œè·³è¿‡")
                        pass
            # å¤„ç†å•è¯è¡Œ
            else:
                if re.search(r"[\u4e00-\u9fff]", line):
                    word = None  # è·³è¿‡åŒ…å«ä¸­æ–‡çš„è¡Œ
                    continue
                else:
                    word = line.strip()

    # æäº¤äº‹åŠ¡å¹¶å…³é—­è¿æ¥
    conn.commit()
    conn.close()

    handle_linked_words(oald_txt, phoneticsdb_file)


def handle_linked_words(oald_txt: Path, phoneticsdb_file: Path):
    """
    å¤„ç† oald-fork.txt æ–‡ä»¶ä¸­çš„é“¾æ¥è¯æ¡
    """
    if not oald_txt.exists():
        logger.error(f"{oald_txt} æœªæ‰¾åˆ°")
        raise FileNotFoundError(f"{oald_txt} æœªæ‰¾åˆ°")
    if not phoneticsdb_file.exists():
        logger.error(f"{phoneticsdb_file} æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
        raise FileNotFoundError(f"{phoneticsdb_file} æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
    # è¿æ¥åˆ° phonetics.ddb æ•°æ®åº“
    conn = duckdb.connect(phoneticsdb_file)
    cursor = conn.cursor()

    with oald_txt.open("r", encoding="utf-8") as f:
        word = None  # å½“å‰è¯æ¡
        # é€è¡Œè¯»å–æ–‡ä»¶
        for line in f:
            line = line.strip()  # å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦
            if not line:
                continue  # è·³è¿‡ç©ºè¡Œ
            # å¤„ç† "</>" è¡Œ
            if line.startswith("</>"):
                word = None  # é‡ç½®å½“å‰è¯æ¡
                continue
            # å¤„ç† "@@@LINK=" è¡Œ
            elif line.startswith("@@@LINK="):
                # ä» phonetics.ddb å– phon_uk å’Œ phon_us
                linked_to = line.lstrip("@@@LINK=")
                cursor.execute(
                    """
                    SELECT phon_uk, phon_us
                    FROM words
                    WHERE word = ?""",
                    (linked_to,),
                )
                result = cursor.fetchone()

                if result:
                    (phon_uk, phon_us) = result
                    # æ›´æ–° phoneticsdb_file ä¸­çš„ phon_uk å’Œ phon_am åˆ—
                    if phon_uk is not None and phon_us is not None:
                        # å°†å•è¯å’ŒéŸ³æ ‡ä¿¡æ¯æ’å…¥æ•°æ®åº“
                        try:
                            cursor.execute(
                                """
                                INSERT INTO words (word, phon_uk, phon_us)
                                VALUES (?, ?, ?)
                            """,
                                (word, phon_uk, phon_us),
                            )
                        except duckdb.duckdb.ConstraintException:
                            logger.debug(f"é“¾æ¥è¯æ¡ {linked_to} å·²å­˜åœ¨ï¼Œè·³è¿‡")
                            pass
            # å¤„ç†æ™®é€šè¯æ¡è¡Œ
            elif line.startswith("<link href="):
                word = None  # è·³è¿‡æ™®é€šè¯æ¡
                continue
            # å¤„ç†å•è¯è¡Œ
            else:
                if re.search(r"[\u4e00-\u9fff]", line):
                    word = None  # è·³è¿‡åŒ…å«ä¸­æ–‡çš„è¡Œ
                    continue
                else:
                    word = line.strip()

    # æäº¤äº‹åŠ¡å¹¶å…³é—­è¿æ¥
    conn.commit()
    conn.close()


def convert_stardictdb_to_txt(
    stardictdb_file: Path, txt_file: Path, buffer_size: int = 1_000
):
    """
    ä» stardict.ddb å–æ•°æ®å¹¶ç”ŸæˆæŒ‡å®šæ ¼å¼çš„ HTML å†…å®¹ï¼Œç„¶åå°†å…¶å†™å…¥ stardict.txt æ–‡ä»¶ï¼Œç”¨äºç”Ÿæˆmdxè¯å…¸æ–‡ä»¶
    :param stardictdb_file: stardict.ddb æ•°æ®åº“æ–‡ä»¶è·¯å¾„
    :param txt_file: è¾“å‡ºçš„ stardict.txt æ–‡ä»¶è·¯å¾„
    :param buffer_size: ç¼“å†²åŒºå¤§å°ï¼Œè¡¨ç¤ºç¼“å­˜å¤šå°‘æ¡ HTML å†…å®¹åå†™å…¥æ–‡ä»¶ï¼Œé»˜è®¤ä¸º 1000
    """
    if not stardictdb_file.exists():
        logger.error(f"{stardictdb_file} æœªæ‰¾åˆ°")
        raise FileNotFoundError(f"{stardictdb_file} æœªæ‰¾åˆ°")
    if txt_file.exists():
        logger.error(f"{txt_file} å·²å­˜åœ¨")
        raise FileExistsError(f"{txt_file} å·²å­˜åœ¨")

    # è¿æ¥åˆ° stardict.ddb æ•°æ®åº“
    conn = duckdb.connect(database=str(stardictdb_file), read_only=True)
    cursor = conn.cursor()
    query = """SELECT word, phonetic, definition, translation, collins, oxford, tag, bnc, frq, exchange FROM stardict"""
    cursor.execute(query)

    # ä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰æ•°æ®
    rows = cursor.fetchall()

    # å†™å…¥ txt_file æ–‡ä»¶
    with txt_file.open("w", encoding="utf-8") as f:
        buffer = []  # ç”¨äºç¼“å­˜ç”Ÿæˆçš„ HTML å†…å®¹
        for row in rows:
            (
                word,
                phonetic,
                definition,
                translation,
                collins,
                oxford,
                tag,
                bnc,
                frq,
                exchange,
            ) = row

            # ç”Ÿæˆè‹±æ–‡åˆ°ä¸­æ–‡çš„ HTML ç»“æ„
            soup = generate_html(
                word,
                phonetic,
                definition,
                translation,
                collins,
                oxford,
                tag,
                bnc,
                frq,
                exchange,
            )
            buffer.append(str(soup) + "\n")

            # ç”Ÿæˆä¸­æ–‡åˆ°è‹±æ–‡çš„ HTML ç»“æ„
            if translation:
                translations = translation.split("\\n")
                for trans in translations:
                    # æ’é™¤ä¸åŒ…å«ä¸­æ–‡çš„æƒ…å†µ
                    if not CHINESE_PATTERN.search(trans):
                        continue
                    # ä½¿ç”¨ POS_PATTERN æå–è¯æ€§å’Œç¿»è¯‘å†…å®¹
                    match = POS_PATTERN.match(trans)
                    if match:
                        pos_part = match.group(1) + "."  # è¯æ€§éƒ¨åˆ†ï¼ˆå¦‚ "n."ï¼‰
                        trans_part = match.group(2)  # ç¿»è¯‘å†…å®¹éƒ¨åˆ†
                    else:
                        pos_part = ""
                        trans_part = trans
                    # ç”Ÿæˆä¸­æ–‡åˆ°è‹±æ–‡çš„æ¡ç›®
                    chinese_soup = generate_html(
                        trans_part, "", definition, word, "", "", "", "", "", ""
                    )
                    buffer.append(str(chinese_soup) + "\n")

            # å¦‚æœç¼“å†²åŒºè¾¾åˆ°æŒ‡å®šå¤§å°ï¼Œå†™å…¥æ–‡ä»¶
            if len(buffer) >= buffer_size:
                f.write("".join(buffer))  # æ‰¹é‡å†™å…¥
                buffer.clear()  # æ¸…ç©ºç¼“å†²åŒº

        # å†™å…¥å‰©ä½™çš„ç¼“å†²åŒºå†…å®¹
        if buffer:
            f.write("".join(buffer))

    # å…³é—­æ¸¸æ ‡å’Œè¿æ¥
    cursor.close()
    conn.close()


def generate_html(
    word,
    phonetic,
    definition,
    translation,
    collins,
    oxford,
    tag,
    bnc,
    frq,
    exchange,
):
    """
    ç”Ÿæˆ HTML ç»“æ„
    """
    global POS_PATTERN

    soup = BeautifulSoup(features="html.parser")
    html_tag = soup.new_tag("html")
    soup.append(html_tag)

    head_tag = soup.new_tag("head")
    html_tag.append(head_tag)

    title_tag = soup.new_tag("title")
    title_tag.string = word
    head_tag.append(title_tag)

    link_tag = soup.new_tag(
        "link", href="concise-enhanced.css", rel="stylesheet", type="text/css"
    )
    head_tag.append(link_tag)

    body_tag = soup.new_tag("body")
    html_tag.append(body_tag)

    div_bdy = soup.new_tag("div", **{"class": "bdy", "id": "ecdict"})
    body_tag.append(div_bdy)

    div_ctn = soup.new_tag("div", **{"class": "ctn", "id": "content"})
    div_bdy.append(div_ctn)

    div_hwd = soup.new_tag("div", **{"class": "hwd"})
    div_hwd.string = word
    div_ctn.append(div_hwd)

    hr_hrz = soup.new_tag("hr", **{"class": "hrz"})
    div_ctn.append(hr_hrz)

    # git(ipa hnt oxf col)  phonetic  oxford  collins
    if phonetic or collins or oxford:
        div_git = soup.new_tag("div", **{"class": "git"})
        div_ctn.append(div_git)
        if phonetic:
            span_ipa = soup.new_tag("span", **{"class": "ipa"})
            span_ipa.string = f"[{phonetic}]"
            div_git.append(span_ipa)

        if collins or oxford:
            span_hnt = soup.new_tag("span", **{"class": "hnt"})
            span_hnt.string = "-"
            div_git.append(span_hnt)
            if oxford:
                span_oxf = soup.new_tag(
                    "span", **{"class": "oxf", "title": "Oxford 3000 Keywords"}
                )
                span_oxf.string = "â€»" * int(oxford)
                div_git.append(span_oxf)

            if collins:
                span_col = soup.new_tag(
                    "span", **{"class": "col", "title": "Collins Stars"}
                )
                span_col.string = "â˜…" * int(collins)
                div_git.append(span_col)

    # gdc(dcb dcb ...)  translation
    # dcb(pos dcn) / dcb(dnt dne)  è¯æ€§ ç¿»è¯‘ / [ç½‘ç»œ] ç¿»è¯‘
    if translation:
        div_gdc = soup.new_tag("div", **{"class": "gdc"})
        div_ctn.append(div_gdc)

        # æŒ‰æ¢è¡Œç¬¦åˆ†å‰²ç¿»è¯‘å†…å®¹
        translations = translation.split("\\n")
        for trans in translations:
            # åŒ¹é…è¯æ€§å’Œç¿»è¯‘å†…å®¹
            match = POS_PATTERN.match(trans)
            if match:
                pos_part = match.group(1) + "."  # è¯æ€§éƒ¨åˆ†ï¼ˆå¦‚ "n."ï¼‰
                trans_part = match.group(2)  # ç¿»è¯‘å†…å®¹éƒ¨åˆ†
            else:
                pos_part = ""
                trans_part = trans

            # åˆ›å»ºç¿»è¯‘æ¡ç›®çš„ HTML ç»“æ„
            div_dcb = soup.new_tag("span", **{"class": "dcb"})

            # åŒ¹é… dcb(dnt dne)  [ç½‘ç»œ] ç¿»è¯‘
            if trans_part.startswith("[ç½‘ç»œ]"):
                span_dnt = soup.new_tag("span", **{"class": "dnt"})
                span_dnt.string = "[ç½‘ç»œ]"
                div_dcb.append(span_dnt)
                span_dne = soup.new_tag("span", **{"class": "dne"})
                span_dne.string = trans_part.lstrip("[ç½‘ç»œ]")
                div_dcb.append(span_dne)
            else:
                # å¦‚æœæœ‰è¯æ€§ï¼Œæ·»åŠ è¯æ€§éƒ¨åˆ†
                if pos_part:
                    span_pos = soup.new_tag("span", **{"class": "pos"})
                    span_pos.string = pos_part
                    div_dcb.append(span_pos)

                # æ·»åŠ ç¿»è¯‘å†…å®¹éƒ¨åˆ†
                span_dcn = soup.new_tag("span", **{"class": "dcn"})
                span_dcn.string = trans_part
                div_dcb.append(span_dcn)

            # å°†ç¿»è¯‘æ¡ç›®æ·»åŠ åˆ° gdc å®¹å™¨ä¸­
            div_gdc.append(div_dcb)

            # æ·»åŠ æ¢è¡Œæ ‡ç­¾
            br_tag = soup.new_tag("br")
            div_gdc.append(br_tag)
    else:
        if definition:
            # æ²¡æœ‰ä¸­æ–‡ç¿»è¯‘å†…å®¹ï¼Œä½†æœ‰è‹±æ–‡å®šä¹‰ï¼Œåˆ™ç”Ÿæˆè‹±æ–‡åˆ°è‹±æ–‡çš„æ¡ç›®
            # gcd(dcb dcb ...)  definition
            # dcb(dcn) / dcb(deq)
            div_gdc = soup.new_tag("div", **{"class": "gdc"})
            div_ctn.append(div_gdc)
            definitions = definition.split("\\n")
            for defi in definitions:
                # åˆ›å»ºç¿»è¯‘æ¡ç›®çš„ HTML ç»“æ„
                div_dcb = soup.new_tag("span", **{"class": "dcb"})
                if defi.startswith(">"):
                    span_deq = soup.new_tag("span", **{"class": "deq"})
                    span_deq.string = defi
                    div_dcb.append(span_deq)
                else:
                    span_dcn = soup.new_tag("span", **{"class": "dcn"})
                    span_dcn.string = defi
                    div_dcb.append(span_dcn)
                # å°†ç¿»è¯‘æ¡ç›®æ·»åŠ åˆ° gdc å®¹å™¨ä¸­
                div_gdc.append(div_dcb)

                # æ·»åŠ æ¢è¡Œæ ‡ç­¾
                br_tag = soup.new_tag("br")
                div_gdc.append(br_tag)

    # gfm()  exchange
    if exchange:
        div_gfm = soup.new_tag("div", **{"class": "gfm"})
        div_ctn.append(div_gfm)
        exchange_dict = {}
        exchanges = exchange.split("/")
        for exch in exchanges:
            type_part, word_part = exch.split(":", 1)
            if type_part not in exchange_dict:
                exchange_dict[type_part] = []
            exchange_dict[type_part].append(word_part)
        if (  # æ—¶æ€
            "p" in exchange_dict
            or "d" in exchange_dict
            or "i" in exchange_dict
            or "3" in exchange_dict
        ):
            div_fmb = soup.new_tag("div", **{"class": "fmb"})
            div_fnm = soup.new_tag("span", **{"class": "fnm"})
            div_fnm.string = "æ—¶æ€:"
            div_fmb.append(div_fnm)
            div_frm = soup.new_tag("span", **{"class": "frm"})
            div_frm.string = ", ".join(
                exchange_dict.get("p", [])
                + exchange_dict.get("d", [])
                + exchange_dict.get("i", [])
                + exchange_dict.get("3", [])
            )
            div_fmb.append(div_frm)
            div_gfm.append(div_fmb)
        if (  # çº§åˆ«
            "r" in exchange_dict or "t" in exchange_dict
        ):
            div_qmb = soup.new_tag("div", **{"class": "qmb"})
            div_qnm = soup.new_tag("span", **{"class": "qnm"})
            div_qnm.string = "çº§åˆ«:"
            div_qmb.append(div_qnm)
            div_qrm = soup.new_tag("span", **{"class": "qrm"})
            div_qrm.string = ", ".join(
                exchange_dict.get("r", []) + exchange_dict.get("t", [])
            )
            div_qmb.append(div_qrm)
            div_gfm.append(div_qmb)
        if (  # åŸå‹
            "0" in exchange_dict and "1" in exchange_dict
        ):
            div_orb = soup.new_tag("div", **{"class": "orb"})
            div_onm = soup.new_tag("span", **{"class": "onm"})
            div_onm.string = "åŸå‹:"
            div_orb.append(div_onm)
            div_orm = soup.new_tag("span", **{"class": "orm"})
            # ä½¿ç”¨åˆ—è¡¨å­˜å‚¨ what å†…å®¹
            what_list = []
            # æ ¹æ® exchange_dict çš„ "1" é”®å€¼å¡«å…… what_list
            for key in exchange_dict.get("1", []):
                for char in key:
                    match char:
                        case "p":
                            what_list.append("è¿‡å»å¼")
                        case "d":
                            what_list.append("è¿‡å»åˆ†è¯")
                        case "i":
                            what_list.append("ç°åœ¨åˆ†è¯")
                        case "3":
                            what_list.append("ç¬¬ä¸‰äººç§°å•æ•°")
                        case "r":
                            what_list.append("æ¯”è¾ƒçº§")
                        case "t":
                            what_list.append("æœ€é«˜çº§")
                        case "s":
                            what_list.append("å¤æ•°å½¢å¼")
            # å°† what_list ç”¨ "å’Œ" è¿æ¥èµ·æ¥
            what_str = "å’Œ".join(what_list)  # ä½¿ç”¨ä¸­æ–‡é¡¿å·è¿æ¥
            if what_str:
                div_orm.string = (
                    f"{word} æ˜¯ {str(exchange_dict.get('0', [])[0])} çš„{what_str}"
                )
            else:
                div_orm.string = f"{word} çš„åŸå‹æ˜¯ {str(exchange_dict.get('0', [])[0])}"
            div_orb.append(div_orm)
            div_gfm.append(div_orb)

    # frq (tag frq/bnc)
    if tag or frq or bnc:
        div_frq = soup.new_tag(
            "div", **{"class": "frq", "title": f"COCA: {frq}, BNC: {bnc}"}
        )
        tag_list = str(tag).split(" ")
        tag_part = ""
        if "zk" in tag_list:
            tag_part += "ä¸­"
        if "gk" in tag_list:
            tag_part += "é«˜"
        if "ky" in tag_list:
            tag_part += "ç ”"
        if "cet4" in tag_list:
            tag_part += "å››"
        if "cet6" in tag_list:
            tag_part += "å…­"
        if "toefl" in tag_list:
            tag_part += "æ‰˜"
        if "ielts" in tag_list:
            tag_part += "é›…"
        if "gre" in tag_list:
            tag_part += "å®"
        if frq or bnc:
            div_frq.string = f"{tag_part} {frq}/{bnc}"
        else:
            div_frq.string = f"{tag_part}"
        div_ctn.append(div_frq)

    hr_hr2 = soup.new_tag("hr", **{"class": "hr2"})
    div_ctn.append(hr_hr2)

    return soup


def generate_mdx(txt_file: Path, mdx_file: Path):
    """è¯»å– stardict.txt æ–‡ä»¶å¹¶è§£æè¯æ¡ï¼Œç”Ÿæˆ concise-enhanced.mdx è¯å…¸æ–‡ä»¶"""
    dictionary = {}

    with txt_file.open("r", encoding="utf-8") as f:
        content = f.read()

        # æŒ‰ </html> åˆ†å‰²è¯æ¡
        entries = content.split("</html>")
        for entry in entries:
            entry = entry.strip()
            if not entry:
                continue  # è·³è¿‡ç©ºå†…å®¹

            # æå–è¯æ¡å’Œ HTML å†…å®¹
            if "<title>" in entry:
                # æå–è¯æ¡
                start = entry.find("<title>") + len("<title>")
                end = entry.find("</title>")
                word = entry[start:end].strip()

                # æå–å®Œæ•´çš„ HTML å†…å®¹
                html_content = entry + "</html>"

                # å°†è¯æ¡å’Œ HTML å†…å®¹å­˜å…¥å­—å…¸
                dictionary[word] = html_content

    # ä½¿ç”¨ MDictWriter ç”Ÿæˆ MDX æ–‡ä»¶
    writer = MDictWriter(
        dictionary,
        title="è‹±æ±‰æ±‰è‹±å­—å…¸",
        description="<font size=5 color=red>ç®€æ˜è‹±æ±‰æ±‰è‹±å­—å…¸å¢å¼ºç‰ˆ - CSS ï¼š20250112<br>"
        "(æ•°æ®ï¼šhttp://github.com/skywind3000/ECDICT)<br>"
        "1. å¼€æºè‹±æ±‰å­—å…¸ï¼šMIT / CC åŒåè®®<br>"
        "2. æ ‡æ³¨ç‰›æ´¥ä¸‰åƒå…³é”®è¯ï¼šéŸ³æ ‡å Kå­—ç¬¦<br>"
        "3. æŸ¯æ—æ–¯æ˜Ÿçº§è¯æ±‡æ ‡æ³¨ï¼šéŸ³æ ‡å 1-5çš„æ•°å­—<br>"
        "4. æ ‡æ³¨ COCA/BNC çš„è¯é¢‘é¡ºåº<br>"
        "5. æ ‡æ³¨è€ƒè¯•å¤§çº²ä¿¡æ¯ï¼šä¸­é«˜ç ”å››å…­æ‰˜é›… ç­‰<br>"
        "6. å¢åŠ æ±‰è‹±åæŸ¥<br>"
        "</font>",
    )

    # å†™å…¥ concise-enhanced.mdx æ–‡ä»¶
    with mdx_file.open("wb") as outfile:
        writer.write(outfile)


def calculate_time_interval(log1, log2):
    # æå–æ—¶é—´æˆ³éƒ¨åˆ†
    time_format = "%Y-%m-%d %H:%M:%S"
    timestamp1 = log1.split(" | ")[0]
    timestamp2 = log2.split(" | ")[0]

    # å°†æ—¶é—´æˆ³è½¬æ¢ä¸º datetime å¯¹è±¡
    time1 = datetime.strptime(timestamp1, time_format)
    time2 = datetime.strptime(timestamp2, time_format)

    # è®¡ç®—æ—¶é—´å·®
    time_diff = time2 - time1

    # æå–å°æ—¶ã€åˆ†é’Ÿã€ç§’
    total_seconds = int(time_diff.total_seconds())
    hours = total_seconds // 3600
    minutes = (total_seconds % 3600) // 60
    seconds = total_seconds % 60

    # æ„å»ºç»“æœå­—ç¬¦ä¸²ï¼Œå¿½ç•¥å€¼ä¸º 0 çš„éƒ¨åˆ†
    result = []
    if hours > 0:
        result.append(f"{hours}h")
    if minutes > 0:
        result.append(f"{minutes}m")
    if seconds > 0:
        result.append(f"{seconds}s")

    # å¦‚æœæ‰€æœ‰å€¼éƒ½ä¸º 0ï¼Œè¿”å› 1s
    if not result:
        return "â±ï¸ 1s"

    return f"â±ï¸ {''.join(result)}"


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—  å®Œæ•´æµç¨‹è€—æ—¶  â±ï¸2h3m26s
    configure_logging("logs", level="DEBUG")

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # 0ï¸âƒ£ ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•ã€å®šä¹‰æ–‡ä»¶è·¯å¾„  â±ï¸1s
    logger.info("åˆ›å»ºè¾“å‡ºç›®å½•ã€å®šä¹‰æ–‡ä»¶è·¯å¾„...")
    # æºæ–‡ä»¶
    csv_file = Path("stardict.csv")
    oald_txt = Path().cwd() / "oald-fork" / "oald-fork.txt"
    # GoldenDict è·¯å¾„
    goldendict_exe = Path(r"C:\SSS\GoldenDict-ng\goldendict.exe")
    # è¾“å‡ºæ–‡ä»¶
    output_dir = Path("output")
    stardictdb_file = Path() / output_dir / "stardict.ddb"
    phoneticsdb_file = Path() / output_dir / "phonetics.ddb"
    txt_file = Path() / output_dir / "stardict.txt"
    mdx_file = Path("concise-enhanced.mdx")
    # æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not csv_file.exists():
        raise FileNotFoundError(
            f"{csv_file} æœªæ‰¾åˆ° (stardict.csv ç”± stardict.7z è§£åŒ…è·å¾—)"
        )
    if not oald_txt.exists():
        print(
            f"{oald_txt} æœªæ‰¾åˆ° ï¼ˆoald-fork.txt ç”± ç²¾è£…ç‰›æ´¥å mdx ä½¿ç”¨ AutoMdxBuilder è§£åŒ…è·å¾—ï¼‰"
        )
    # æ£€æŸ¥ GoldenDict æ˜¯å¦å­˜åœ¨
    if not goldendict_exe.exists():
        raise FileNotFoundError(f"{goldendict_exe} æœªæ‰¾åˆ° (GoldenDict-ng è½¯ä»¶)")
    # æ¸…ç©ºè¾“å‡ºç›®å½•
    if not output_dir.exists():
        output_dir.mkdir()
    if stardictdb_file.exists():
        # åˆ é™¤æ—§çš„è¯å…¸æ•°æ®åº“æ–‡ä»¶
        stardictdb_file.unlink()
    if phoneticsdb_file.exists():
        # åˆ é™¤æ—§çš„éŸ³æ ‡æ•°æ®åº“æ–‡ä»¶
        phoneticsdb_file.unlink()
    if txt_file.exists():
        # åˆ é™¤æ—§çš„ TXT æ–‡ä»¶
        txt_file.unlink()
    if mdx_file.exists():
        # åˆ é™¤æ—§çš„ MDX æ–‡ä»¶
        mdx_file.unlink()
    logger.info("è¾“å‡ºç›®å½•ã€æ–‡ä»¶è·¯å¾„é…ç½®å®Œæˆ")

    # è®°å½•æ­¥éª¤0ç»“æŸæ—¶é—´
    step0_end_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    time_interval = calculate_time_interval(start_time, step0_end_time)
    logger.success(f"æ­¥éª¤0å®Œæˆï¼Œè€—æ—¶: {time_interval}")

    logger.info("å¼€å§‹è½¬æ¢...")

    # 1ï¸âƒ£ â­ ç”Ÿæˆ stardict.ddb  â±ï¸1s
    logger.info("ç”Ÿæˆ stardict.ddb æ–‡ä»¶...")
    convert_csv_to_stardictdb(csv_file, stardictdb_file)
    logger.info(f"stardict.ddb æ–‡ä»¶å·²ç”Ÿæˆï¼š{stardictdb_file}")

    # è®°å½•æ­¥éª¤1ç»“æŸæ—¶é—´
    step1_end_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    time_interval = calculate_time_interval(step0_end_time, step1_end_time)
    logger.success(f"æ­¥éª¤1å®Œæˆï¼Œè€—æ—¶: {time_interval}")

    # 2ï¸âƒ£ ğŸ”– ç”Ÿæˆ phonetics.ddb  â±ï¸14m50s
    logger.info("ç”Ÿæˆ phonetics.ddb æ–‡ä»¶...")
    build_phonetics_ddb(oald_txt, phoneticsdb_file)
    logger.info(f"phonetics.ddb æ–‡ä»¶å·²ç”Ÿæˆï¼š{phoneticsdb_file}")

    # è®°å½•æ­¥éª¤2ç»“æŸæ—¶é—´
    step2_end_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    time_interval = calculate_time_interval(step1_end_time, step2_end_time)
    logger.success(f"æ­¥éª¤2å®Œæˆï¼Œè€—æ—¶: {time_interval}")

    # 3ï¸âƒ£ ğŸ†• æ›´æ–° stardict.ddb éŸ³æ ‡ä¿¡æ¯  â±ï¸1h13m20s
    logger.info("æ›´æ–°éŸ³æ ‡ä¿¡æ¯...")
    update_phonetics_from_phoneticsdb_to_stardictdb(phoneticsdb_file, stardictdb_file)
    logger.info(f"æ›´æ–°éŸ³æ ‡ä¿¡æ¯å®Œæˆï¼š{stardictdb_file}")

    # è®°å½•æ­¥éª¤3ç»“æŸæ—¶é—´
    step3_end_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    time_interval = calculate_time_interval(step2_end_time, step3_end_time)
    logger.success(f"æ­¥éª¤3å®Œæˆï¼Œè€—æ—¶: {time_interval}")

    # 4ï¸âƒ£ ğŸ“„ ç”Ÿæˆ stardict.txt  â±ï¸26m25s
    logger.info("ç”Ÿæˆ stardict.txt æ–‡ä»¶...")
    convert_stardictdb_to_txt(stardictdb_file, txt_file, buffer_size=1_000_000)
    logger.info(f"stardict.txt æ–‡ä»¶å·²ç”Ÿæˆï¼š{txt_file}")

    # è®°å½•æ­¥éª¤4ç»“æŸæ—¶é—´
    step4_end_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    time_interval = calculate_time_interval(step3_end_time, step4_end_time)
    logger.success(f"æ­¥éª¤4å®Œæˆï¼Œè€—æ—¶: {time_interval}")

    # 5ï¸âƒ£ ğŸ“¦ ç”Ÿæˆ concise-enhanced.mdx  â±ï¸8m49s
    logger.info("ç”Ÿæˆ concise-enhanced.mdx æ–‡ä»¶...")
    generate_mdx(txt_file, mdx_file)
    logger.info(f"concise-enhanced.mdx æ–‡ä»¶å·²ç”Ÿæˆï¼š{mdx_file}")

    # è®°å½•æ­¥éª¤5ç»“æŸæ—¶é—´
    step5_end_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    time_interval = calculate_time_interval(step4_end_time, step5_end_time)
    logger.success(f"æ­¥éª¤5å®Œæˆï¼Œè€—æ—¶: {time_interval}")

    # 6ï¸âƒ£ ğŸ” æ‰“å¼€ GoldenDictï¼Œè‡ªåŠ¨é‡å»ºç´¢å¼•  â±ï¸1s
    logger.info("æ‰“å¼€ GoldenDict...")
    import subprocess

    subprocess.Popen(str(goldendict_exe))
    logger.info("GoldenDict å·²æ‰“å¼€")

    # è®°å½•æ­¥éª¤6ç»“æŸæ—¶é—´
    step6_end_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    time_interval = calculate_time_interval(step5_end_time, step6_end_time)
    logger.success(f"æ­¥éª¤6å®Œæˆï¼Œè€—æ—¶: {time_interval}")

    # è®°å½•æ€»è€—æ—¶
    total_time_interval = calculate_time_interval(start_time, step6_end_time)
    logger.success(f"æ‰€æœ‰æ­¥éª¤å®Œæˆï¼Œæ€»è€—æ—¶: {total_time_interval}")
